{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJ271Cky2c8L"
   },
   "source": [
    "# 04 Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Language shapes the way we think, and determines what we can think about.\" ~ Benjamin Lee Whorf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word_cloud](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F491732087%2F960x0.jpg%3Ffit%3Dscale&f=1&nofb=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. What is Natural Language Processing?\n",
    "2. Key Concepts\n",
    "3. What is Latent Dirichlet Allocation?\n",
    "4. Analysis\n",
    "5. Automated Topic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Natural Language Processing and What is Topic Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural Language Processing**\n",
    "\n",
    "> \"Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\" ~ [IBM](https://www.ibm.com/cloud/learn/natural-language-processing)\n",
    "\n",
    "**Topic Modeling**\n",
    "\n",
    "> \"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Topic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a non-exhaustive list of concepts that you should understand, and be familiar with, from the field of Natural Language Processing.\n",
    "\n",
    "- **Corpus** - Collection of documents filled with words, sentences, parragraphs, numbers, punctuations, etc. For example, a collection of letters is a corpus.\n",
    "- **Corpora** - More than one corpus. For example, the collection of job descriptions for a company would be a corpus, the collection of the collection of these companies job descriptions would be a corpora.\n",
    "- **Token** - Element inside a piece of text. This may be a word, a number, a space, any kind of punctuation, etc.\n",
    "- **Tokenization** - separating pieces of strings (i.e. text) into their smallest components or, tokens.\n",
    "- **Document** - A block of text of varying sizes. For example, a document might be a tweet, a menu, a book, a review, etc. \n",
    "- **Bag of Words** - A numerical representation of textual information that a statistical model can understand, process, and make inferences from. In a bag of words, the rows represent the documents in your corpus and the columns represent all of the unique words from all of your documents.\n",
    "- **Topic** - A representation of similar information based words and sometimes context as well.\n",
    "- **Stop Words** - the most common words used in a language. These words appear so often that in many applications of NLP these get removed before the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is Latent Dirichlet Allocation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document.\" ~ [David M. Blei, Andrew Y. Ng and Michael I. Jordan (2003)](https://jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "\n",
    "**Assumptions**\n",
    "- There is some sort of structure in these documents and LDA will try and collapse or separate these structure among your pre-defined set of topics.\n",
    "- Each topic comes from, and can be represented as, a distribution of words or term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at how topic modeling is done with one company and with some base functions, and we will then look at the automated way of searching for a topic.\n",
    "\n",
    "Let's start by importing the packages we will use throughout this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import spacy # one of the best NLP libraries available in any programming language\n",
    "from pprint import pprint # the extra p stands for printing\n",
    "\n",
    "pd.options.display.max_columns = None # this allows us to see all columns displayed after a .head() or .tail() on our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/netflix.csv') # let's read our dataframe\n",
    "df.head() # show the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have quite a few columns but, since we are only interested in the **pros** reviews, let's extract that column out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_reviews = df['pros'].copy() # take the reviews column out of the dataframe\n",
    "pros_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be extracting words that will form a topic, we'll need to do some text preprocessing in order to get rid whatever is not a word. We will also want to have all letters in lowercase and we might want to reduce them their root, if any. To do this, we will use `spacy` which has an English language model ready to use. **Note**, the English language model allows us to use different functionalities on top of English words. The details are not important but note that we now have a tool that will help us wrangle English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # we first load out English language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_review = pros_reviews[418]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(one_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the review above is quite messy and it has a lot of characters that, for all intents and purposes, will not be useful for our analysis. Let's examine a cleaner version of the review above by running it through our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_review = nlp(one_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better and easier to read. Can we examine the sentences as well? You bet we can by using spaCy's many features.\n",
    "\n",
    "Below we will use a loop to go over the index of each sentence of our single review, plus the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the temporary variable num will represent the index\n",
    "# and the temporary variable sentence will represent each line of the review\n",
    "# enumerate is a buil-in Python function\n",
    "\n",
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f\"Sentence #{num}:\\n {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the entities of the words that make up our single review using the same approach as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f\"Entity #{num}: {entity} -- {entity.label_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use additional functionalities to showcase more characteristics about our review. We will do so using list comprehensions. Think of these as loops cousins whose two main differences are that the action happens first and they always return a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are taking out of the parsed review each token\n",
    "token_text = [token.text for token in parsed_review]\n",
    "\n",
    "# here we are lemmatizing each word possible\n",
    "token_lemmas = [token.lemma_ for token in parsed_review]\n",
    "\n",
    "# stopwords are very common so here we will extract a variable that will tell us whether\n",
    "# a word is a stopword or not\n",
    "token_stop = [token.is_stop for token in parsed_review]\n",
    "\n",
    "# we will now add all three to a dataframe and display it without assigning it to a variable\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_stop), columns=['Original Text', 'Lemmatized Text', 'stopwords']).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the middle column above, Lemmatized Text. This column represents the root of some of the words in our review. Think about this as reducing the words with the same meaning but spelled with a different conjugation, to their lowest common denominator. For example, related and relate, reasons and reason, considered and consider, etc. This steps helps us assign the exact word and meaning to the same topic as opposed to differenly spelled words with the same meaning to different topics.\n",
    "\n",
    "Let's now define a function that will return only the punctuations or the trailing space next to some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puncs_out(token): return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to import our stopwords from spaCy to be able to filter them out from our reviews. We don't want `the`, `a`, `so`, etc. influencing our topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's create a function that will remove the punctuations, spaces, and also lemmatize the words in our reviews at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_in_stopw_out(doc):\n",
    "    \"\"\"\n",
    "    This function takes in a piece of text, tokenizes it,\n",
    "    lemmatizes it, removes punctuations and spaces, takes\n",
    "    all stopwords out, and returns the clean piece of text.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = nlp(doc)\n",
    "    tokens_lemma = [token.lemma_ for token in tokens if not puncs_out(token)]\n",
    "    tokens_clean = [token for token in tokens_lemma if token not in STOP_WORDS]\n",
    "    return ' '.join(tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pandas' convenient `.apply()` method to pass in our function above to each of the reviews we have. But first we will make every word in our reviews lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_revs = pros_reviews.str.lower()\n",
    "ready_revs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# the function is called a magic method and it allows us to see how long this cell took to run\n",
    "\n",
    "ready_revs = ready_revs.apply(lemma_in_stopw_out)\n",
    "ready_revs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't want the company we are analysing reviews for to appear in the topics, we will remove it from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we access the string and create a mask of True's and False' where the company appears\n",
    "netflix_mask = ready_revs.str.contains('netflix')\n",
    "netflix_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can filter a dataset by passing in the mask through square brackets []\n",
    "# notice the index\n",
    "ready_revs[netflix_mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a ~ in front of the mask gives us the opposite results\n",
    "ready_revs[~netflix_mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how the word netflix has now dissapeared from the reviews\n",
    "ready_revs[netflix_mask] = ready_revs[netflix_mask].str.replace('netflix', '', regex=False)\n",
    "ready_revs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the differences between and after our preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original:')\n",
    "print('-' * 30)\n",
    "print(nlp(df.loc[148, 'pros']))\n",
    "print()\n",
    "print('Processed Text:')\n",
    "print('-' * 30)\n",
    "print(ready_revs[148])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a bag of words with sklearn's `CountVectorizer()` method. We will remove words that appear less than 4 times, as well as those that appear in 95% of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we instantiate the vectorizer\n",
    "vectorizer = CountVectorizer(min_df=3, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we fit and transform our clean reviews\n",
    "bow = vectorizer.fit_transform(ready_revs)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the output of our bag of words. This is called a sparse matrix and is an efficient way of holding large amounts of 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a topic\n",
    "topics = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate our LDA model with the topics selected above and the fit our sparse matrix to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=topics, # number of topics\n",
    "                                      max_iter=100, # these are the amount of times the algorithm will run\n",
    "                                      learning_method='online', \n",
    "                                      random_state=42, # setting a seed for reproducible results\n",
    "                                      n_jobs=-1) # this parameter makes sure we use all of the cores in our machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the bag of words\n",
    "lda_model.fit(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we just ran our first model so let's go ahead and create a function to evaluate the topics we extracted and see if these make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words=15):\n",
    "    \"\"\"\n",
    "    This function takes our vectorizer, our model, and a\n",
    "    number of words to display the topics from our model.\n",
    "    \"\"\"\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's evaluate the topics\n",
    "show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up we will create a variable with the names of the words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method create an array with the words/keys\n",
    "terms = sorted(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now create a dataframe with our ba\n",
    "bow_docs = pd.DataFrame(bow.toarray(), columns=terms)\n",
    "bow_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the proportion of a word given the topic(s) it fell under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(lda_model.components_.T, index=terms, columns=['topic_' + str(i) for i in range(topics)])\n",
    "components.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to get topics given a model, let's automate the search of the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automated Topic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "# nlkt.download('punkt')\n",
    "\n",
    "import koolture as kt\n",
    "\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean_gs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_of_interest = df.employer.value_counts()\n",
    "comps_of_interest.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_of_interest = (comps_of_interest[(comps_of_interest == 48)]).index\n",
    "len(comps_of_interest), comps_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = df['employer'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['employer'].unique() # get the unique IDs or unique employers in the dataset\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_nums = df_interest['employer'].value_counts().reset_index()\n",
    "reviews_nums.columns = ['employerID', 'reviews_nums']\n",
    "reviews_nums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Custom Stopwords List Before Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel. You first normalize the reviews and then take the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pros = df_interest['pros'].values\n",
    "stopwords = nltk.corpus.stopwords.words('english') + [token.lower() for token in unique_ids]\n",
    "stopwords[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_doc = partial(kt.normalize_doc, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(kt.root_of_word, data_pros_cleaned))\n",
    "\n",
    "df_interest['pros_clean'] = data_pros_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorizers Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers_dicts = kt.get_vectorizers(data=df_interest, unique_ids=unique_ids,\n",
    "                                       company_col='employer', reviews_col='pros', \n",
    "                                       vrizer=CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block run the models in parallel over the companies available and using the specifiedamount of topics in our_range variable and return a dictionary with the output of the get_models function for each company. It is used to identify the interval to search further for optimal topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "partial_func = partial(kt.get_models, topics=our_range, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output = list(e.map(partial_func, unique_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will now iterate over the dictionary output from above, add each dataset into a list, and then concatenate them all into one dataset (output df contains exactly same information, but more readable, and used in next blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = kt.build_dataframe(output)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates over the new dataframe, searches for the top 2 topics based on highest coherence, and appends to a list a tuple containing the company, a tuple with the top two topic numbers, and the fitted vectorizer from the original `vectorizers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topics_sorted, comps, tops = kt.top_two_topics(data=output_df, companies_var='company',\n",
    "                               coherence_var='coherence', topics_var='topics',\n",
    "                               unique_ids=unique_ids, vrizers_list=vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `get_models` function again over the new space of topics. You will  need to\n",
    "1. sort the tuple with the top two topics.\n",
    "2. create a linearly spaced array with 10 elements between the top 2 topics, turn it into integers, make the array a set to eliminate any duplicates that might arise if there is a 2 in the top two topics, and then turn that into a list.\n",
    "3. get your fixed partial function again\n",
    "4. the output is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "partial_func = partial(kt.get_models, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output2 = list(e.map(partial_func, comps, tops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple dataframes from dictionaries again and collapse them into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df2 = kt.build_dataframe(output2)\n",
    "output_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best topic based on the new output, and get the top 10 words per topic. At the moment, you are only adding 1 of the topics for each company but you can change this by removing the indexing in `top_topics` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_topics = kt.absolute_topics(output_df2, 'company', 'coherence', \n",
    "                                 'topics', 'models', vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your output. Get the probabilities dataframes for each company and add them to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "for tup in vectorizers_dicts.values():\n",
    "    docs_of_probas[tup[0]] = pd.DataFrame(best_topics[tup[0]][1].transform(tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Measures of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "comP_h_results = defaultdict(float)\n",
    "comT_h_results = defaultdict(float)\n",
    "entropy_avg_results = defaultdict(float)\n",
    "cross_entropy_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comP_h_results[company] = kt.comph(proba_df.values)\n",
    "    comT_h_results[company] = kt.conth(proba_df)\n",
    "    entropy_avg_results[company] = kt.ent_avg(proba_df.values)\n",
    "    cross_entropy_results[company] = kt.avg_crossEnt(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comph_df = pd.DataFrame.from_dict(comP_h_results.items())\n",
    "conth_df = pd.DataFrame.from_dict(comT_h_results.items())\n",
    "crossEnt_df = pd.DataFrame.from_dict(cross_entropy_results.items())\n",
    "cultureMetrics = comph_df.merge(conth_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics = cultureMetrics.merge(crossEnt_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics.columns = ['employerID', 'comph', 'conth', 'avgCrossEnt']\n",
    "cultureMetrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_topics = pd.DataFrame.from_records(best_topics).T.reset_index()\n",
    "df_best_topics.columns = ['employerID', 'best_topic', 'model', 'coherence']\n",
    "df_best_topics.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LDA&PBTI_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "people",
   "language": "python",
   "name": "people"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
